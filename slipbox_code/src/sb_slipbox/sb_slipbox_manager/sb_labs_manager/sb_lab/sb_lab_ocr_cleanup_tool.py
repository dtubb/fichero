  #src/sb_slipbox_manager/sb_labs_manager/sb_lab/sb_lab_ocr_cleanup_tool.py

"""
The sb_extracts_assistant is an agent focused on reviewing and improving extracted text from source documents. 

It is initialized by the sb_lab_director with the extracted text and original source file. 

The assistant's key task is quality assurance - reviewing OCR output or other extracted text, and improving it by fixing errors, without fabricating content.

Key responsibilities:

- Receive extracted text and source file from director 
- Analyze text for potential OCR errors
- Identify misspellings, formatting issues, etc.
- Correct minor OCR errors, without changing meaning
- Flag problematic sections, requiring careful review
- Return revised extract to director

This allows leveraging LLMs to clean up extracted text before it is processed into slips. By handling quality assurance, the extracts assistant provides cleaner input for downsteam agents.

It focuses narrowly on text improvements to complement other assistants. Together they build up the networked slipbox from sources.
"""

from src.common import *
from .sb_lab_assistant import sb_lab_assistant

from langchain.schema import AgentAction, AgentFinish

from langchain import hub
from langchain.agents import create_openai_functions_agent
from langchain_openai.chat_models import ChatOpenAI

from langchain_core.runnables import RunnablePassthrough
from langchain_core.agents import AgentFinish

from langgraph.graph import END, Graph

from langchain.globals import set_debug
from langchain.globals import set_verbose

from langchain.agents import Tool
from langchain.agents import AgentType, initialize_agent, load_tools

from langchain.tools import StructuredTool  
from langchain.prompts import PromptTemplate

from langchain.chains.llm import LLMChain
from langchain_core.output_parsers import StrOutputParser

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI


# Constants

class sb_lab_ocr_cleanup_tool(sb_lab_assistant):

  def ocr_cleanup_tool_function(self, source):
    
    print("\tOCR_CLEANUP_TOOL > OCR_CLEANUP_TOOL working…")
    
    _TOOL_PROMPT = """You are an AI research assistant in a AI research lab. Your task is to clarify OCR extracted text from a document, which might be an archival document which may have been damaged, have pages torn, or water damage, a research paper, an article, a book, or other source. Your goal is to improve readability while preserving fidelity to the original. The OCR was generated by Google Vision, and it may have sentences out of order. 
    
Your task is to produce a text as faithful as possible to the original OCR extract, with improved clarity but no unsupported changes or invented data. Rely solely on the provided extract as context.

When reviewing the OCR text, apply these guidelines:

- Replace text that is clearly misrecognized as **UNCLEAR**. The OCR does not work on handwriting or script, and some documents are only handwritten. For such text, delete unclear text and mark as **UNCLEAR**
- Do not invent content or make unsupported leaps. Rely solely on the provided text for context.
- Correct any clear OCR errors based on surrounding words and language patterns.
- Remove any irrelevant numbers that appear at the bottom which are a artifact from a ruler.
- Format for better readability but do not alter meaning.
- For any indecipherable or ambiguous words use markdown bold and all caps **UNCLEAR**, only make corrections fully supported by context. Otherwise, leave as is.
- Use **UNCLEAR** placeholders for any apparent gaps in the original document. Avoid fabricating content to fill gaps.
- Preserve original tone, language structure and stylistic elements.
- Focus on clarification.
- Document any uncertainties requiring further human review.

- DO NOT CHANGE THE LANGUAGE OF THE TEXT.
- Do not provide notes at the end or specifically flag things you've deleted, other than with **BOLD TEXT**.
- Do return the original text and the revised text, as follow:
- REMEMBER DO NOT CHANGE THE LANGUAGE OF THE SOURCE. IT IS IN SPANISH. 
- YOUR WORKING LANGUAGE IS SPANISH

REVISED TEXT:

ORIGINAL TEXT: 

The previous input is as follows:

TEXT TO CLEAN UP:
{source}
""" 

    # OCR Cleanup logic  

    prompt = ChatPromptTemplate.from_template(f"{_TOOL_PROMPT}")
    
    """
    # Check the DEFAULT_LLM constant, and use it, and the appropriate model.
    if DEFAULT_LLM == "openai":
        llm = ChatOpenAI(model=f"{DEFAULT_LLM_MODEL}")
        
    elif DEFAULT_LLM == "ollama":
        # Replace 'ollama_engine' with the appropriate call to create an ollama engine instance
        llm = ollama_engine(model=f"{DEFAULT_LLM_MODEL}")
    else:
        raise ValueError("Unsupported LLM engine specified.")    

    log_and_print(f"Initializing {self.get_name()} afent {self.lab.get_name()} lab with LLM {DEFAULT_LLM} and model {DEFAULT_LLM_MODEL}",
                  LOG_LEVELS.INFO, VERBOSITY_LEVELS.INFO)
    """
    llm = ChatOpenAI(model=f"gpt-4")

    output_parser = StrOutputParser()   
    chain = prompt | llm | output_parser
    
    cleaned_text = chain.invoke({"source": f"{source}"})

    return cleaned_text

  # Define the tool function  
  
  def ocr_qa_tool_function(self, source):
    print("\tOCR_CLEANUP_TOOL > OCR_QA_TOOL working…")
  
    _TOOL_PROMPT = """You are an AI research assistant in an AI research lab. 

Your task is to thoroughly validate the corrections made to clarify an text generated by OCR from a document, to ensure that changes remains faithful to the original document and that changes suggested are supported in the context provided.
    
Please compare the corrected text to the original OCR and note any  changes, not unsupported by the context, using these guidelines:

- Compare the two texts, and flag any unsupported changes.
- Make sure the language has not been changed. If it has, make sure to flag to change it back.
- Ensure no new content has been added without support from the context of the original text. Avoid inventing information, but you can acccept corrections supported by the text.
- Review formatting changes so that they enhance readability, but do not alter meaning.
- Verify corrections of common OCR errors like "rn" -> "m" are substantiated.
- Do not make any changes. The goal is validating corrections, not further editing.
- For any identified issues, clearly describe the issue and recommend actions to resolve them. 
- Remember any irrelevant numbers that appear at the bottom which are a artifact from a ruler and can be deleted.
- BE HARSH. Your job is to be an undertake a quality assurance and validation step on the work of another assistant, which was tasked with clarifying the OCR. You task is to make sure the other AI did not go to far. However, you share the goal of a clarified text, so don't flag issues where there are none.
- Make sure no sections where accidentally cut.
- DO NOT CHANGE THE LANGUAGE OF THE TEXT.
- Do return the the quality assurance evaluation, the revised text, and the original text, as follows:
- REMEMBER DO NOT CHANGE THE LANGUAGE OF THE SOURCE.

QUALITY ASSURANCE EVALUATION:

REVISED TEXT:
   
ORIGINAL TEXT:

The previous inputs were as follows:

{source}
"""

    # OCR Cleanup logic  

    prompt = ChatPromptTemplate.from_template(f"{_TOOL_PROMPT}")
    
    """
    # Check the DEFAULT_LLM constant, and use it, and the appropriate model.
    if DEFAULT_LLM == "openai":
        llm = ChatOpenAI(model=f"{DEFAULT_LLM_MODEL}")
        
    elif DEFAULT_LLM == "ollama":
        # Replace 'ollama_engine' with the appropriate call to create an ollama engine instance
        llm = ollama_engine(model=f"{DEFAULT_LLM_MODEL}")
    else:
        raise ValueError("Unsupported LLM engine specified.")    

    log_and_print(f"Initializing {self.get_name()} afent {self.lab.get_name()} lab with LLM {DEFAULT_LLM} and model {DEFAULT_LLM_MODEL}",
                  LOG_LEVELS.INFO, VERBOSITY_LEVELS.INFO)
    
    # I know this works.
    """    
    llm = ChatOpenAI(model=f"gpt-4")
    
    output_parser = StrOutputParser()   
    chain = prompt | llm | output_parser
    
    cleaned_text = chain.invoke({"source": f"{source}"})

    return cleaned_text

  # Define the tool function  
  
  def ocr_implement_qa_suggestions_tool_function(self, source):
  
    print("\tOCR_CLEANUP_TOOL > OCR_IMPLEMENT_QA_SUGGESTIONS_TOOL working…")
    
    _TOOL_PROMPT = """You are an AI research assistant in a research lab.  Your task is to implement the suggestions from a Quality Assurance assistant, which reviewed an earlier version of a cleanup OCR text, You will take the original OCR text, the earlier revisions of the OCR text and the QA suggestions as input, and implement the feedback to improve quality. Please follow these guidelines:

- Carefully review the earlier versions and QA reports and documented discrepancies.
- Make sure the language has not been changed. If it has, make sure to flag to change it back.
- For any flagged issues, make the corrections needed to resolve them, without overcorrecting.
- Only implement changes that are fully substantiated by the context and original text. Avoid guessing if not enough context is provided.
- Use placeholders like **UNCLEAR** when unable to resolve ambiguous text
- Focus narrowly on addressing QA feedback; do not attempt further improvements unprompted.
- If a suggestion seems inappropriate or unjustified, ignore it rather than implementing.
- You all share the gaol of clarifying the text.
- The goal is to integrate only high-confidence corrections needed to resolve QA issues.
- Document change made.
- Apply these directives to efficiently integrate the QA suggestions while preserving fidelity to the original context. Implement only substantiated, minimally invasive changes directly addressing the QA feedback. Maintain the integrity of the text, and flag any unclear or risky suggestions rather than introducing errors.
- Remove any irrelevant numbers that appear at the bottom which are a artifact from a ruler.
- Remember, you and your colleagues are AIs, and there is no point in instructing reviewing the original file. Please provide your colleague the ocr_implement_qa_suggestions_tool actionable steps, that do not work it cannot undertake.
- DO NOT CHANGE THE LANGUAGE OF THE TEXT.
- Do not provide additional commentary.
- Do return the revised text based on quality assurance evaluation, the revised text, and the original text.
- REMEMBER DO NOT CHANGE THE LANGUAGE OF THE SOURCE. IT IS IN SPANISH. 
- YOUR WORKING LANGUAGE IS SPANISH

REVISED TEXT BASED ON QUALITY ASSURANCE EVALUATION:

REVISED TEXT:

ORIGINAL TEXT:

The previous input was as follows:

{source}
"""

    prompt = ChatPromptTemplate.from_template(f"{_TOOL_PROMPT}")

    # Check the DEFAULT_LLM constant, and use it, and the appropriate model.
    if DEFAULT_LLM == "openai":
        llm = ChatOpenAI(model=f"{DEFAULT_LLM_MODEL}")
        
    elif DEFAULT_LLM == "ollama":
        # Replace 'ollama_engine' with the appropriate call to create an ollama engine instance
        llm = ollama_engine(model=f"{DEFAULT_LLM_MODEL}")
    else:
        raise ValueError("Unsupported LLM engine specified.")    

    log_and_print(f"Initializing {self.get_name()} afent {self.lab.get_name()} lab with LLM {DEFAULT_LLM} and model {DEFAULT_LLM_MODEL}",
                  LOG_LEVELS.INFO, VERBOSITY_LEVELS.INFO)

    output_parser = StrOutputParser()   
    chain = prompt | llm | output_parser
    
    cleaned_text = chain.invoke({"source": f"{source}"})

    return cleaned_text
  
  def ocr_cleanup(self, source):
  
    print("\t+++++++++++++++++++++++++++++++++++")
    print("\tOCR_CLEANUP_TOOL STARTING UP")

    # Gotta Define the tools for the LanGgraph for this OCR Cleanup Tookl
    print("\tDefining LAB > OCR_CLEANUP_TOOL > ocr_cleanup_tool")
    self.ocr_cleanup_tool = StructuredTool.from_function(
      name="ocr_cleanup_tool",
      func=self.ocr_cleanup_tool_function,
      description="Cleans up OCR extracted text by correcting minor errors and improving readability while preserving original meaning and context. Requires TEXT TO CLEANU UP.",
      handle_tool_error=True,
      verbose=True
    )

    print("\tDefining LAB > OCR_CLEANUP_TOOL > ocr_qa_tool")
    self.ocr_qa_tool = StructuredTool.from_function(
      name="ocr_qa_tool",
      func=self.ocr_qa_tool_function,
      description="Performs a quality assurance check on OCR text revisions, offering suggestions for enhancement. Input needed: REVISED TEXT and ORIGINAL TEXT.",
      handle_tool_error=True,
      verbose=True  
    )

    print("\tDefining LAB > OCR_CLEANUP_TOOL > ocr_implement_qa_suggestions_tool")
    self.ocr_implement_qa_suggestions_tool = StructuredTool.from_function(
      name="ocr_implement_qa_suggestions_tool",
      func=self.ocr_implement_qa_suggestions_tool_function,
      description="Implements QA suggestions to further improve OCR text revisions. Input needed QA EVALUATION, REVISED TEXT, and ORIGINAL TEXT.",
      handle_tool_error=True,
      verbose=True
    )

    # Gotta create a Tools array
    self.tools = [
         self.ocr_cleanup_tool,
         self.ocr_qa_tool,
         self.ocr_implement_qa_suggestions_tool
    ]
    
    print(f"{self.tools}")
    self.create_langraph()
    print("LANGRAPH CREATED")
    
    self.workflow.add_node("OCR_Cleanup_Tool", self.ocr_cleanup_tool_function)
    self.workflow.add_node("OCR_QA_Tool", self.ocr_qa_tool_function)
    self.workflow.add_node("OCR_Implement_QA_Suggestions_Tool", self.ocr_implement_qa_suggestions_tool_function)

    # Set the entrypoint as `agent`
    # This means that this node is the first one called
    self.workflow.set_entry_point("agent")

    # We now add a conditional edge
    self.workflow.add_conditional_edges(
    # First, we define the start node. We use `agent`.
    # This means these are the edges taken after the `agent` node is called.
    "agent",
    # Next, we pass in the function that will determine which node is called next.
    self.should_continue,
    # Finally we pass in a mapping.
    # The keys are strings, and the values are other nodes.
    # END is a special node marking that the graph should finish.
    # What will happen is we will call `should_continue`, and then the output of that
    # will be matched against the keys in this mapping.
    # Based on which one it matches, that node will then be called.
    {
        # If `tools`, then we call the tool node.
        "continue": "tools",
        "OCR_Cleanup_Tool": "OCR_Cleanup_Tool",
        "OCR_QA_Tool": "OCR_QA_Tool",
        "OCR_Implement_QA_Suggestions_Tool": "OCR_Implement_QA_Suggestions_Tool",
        # Otherwise we finish.
        "exit": END
      }
    )

    # We now add a normal edge from `tools` to `agent`.
    # This means that after `tools` is called, `agent` node is called next.
    self.workflow.add_edge('tools', 'agent')
    self.workflow.add_edge('OCR_Cleanup_Tool', 'OCR_QA_Tool')
    self.workflow.add_edge('OCR_QA_Tool', 'OCR_Implement_QA_Suggestions_Tool')
    self.workflow.add_edge('OCR_Implement_QA_Suggestions_Tool', 'agent')        
    
    self.compile_langraph()
    final_output = ""
    
    _TOOL_TEMPLATE = f"""Improve the OCR of the text, undertake quality assurance on the REVISED TEXT, and implement recommended changes, while  maintaining fidelity to the original. 
    
You have 3 tools:

1. OCR_Cleanup_Tool: Improves the readability of OCR text
2. OCR_QA_Tool: Performs a quality assurance analysis o revised text
3. OCR_Implement_QA_Suggestions_Tool: Integrates QA feedback on revised text

When invoking each tool, provide the full context of previous tools, for reference by the new tool.

After you are done with the tools, please return only the FINAL REVISED TEXT BASED ON QUALITY ASSURANCE EVALUATION cleaned up OCR text, without the back and forth. Your supervisor just wants the end result, not the full conversation history. 

- REMEMBER DO NOT CHANGE THE LANGUAGE OF THE SOURCE. IT IS IN SPANISH. 
- YOUR WORKING LANGUAGE IS SPANISH

The previous input is as follows:

{source}
"""

    for output in self.chain.stream(
           {"input": f"""{_TOOL_TEMPLATE}""", 
           "intermediate_steps": []}
       ):

       for key, value in output.items():

        print(f"\n\tOutput from OCR_CLEANUP_TOOL Node '{key}':")
        
        # Print original input
        if "input" in value:  
          # print("Original Input:", value["input"])
          pass

        agent_outcome = value.get("agent_outcome")

        if agent_outcome is None:
          continue 

        # Print summary for AgentFinish
        if isinstance(agent_outcome, AgentFinish):
          final_output=agent_outcome.return_values["output"]

        # Print details for AgentAction
        elif isinstance(agent_outcome, AgentAction):
          # print("Agent Prompt:", agent_outcome.log)
          print("\tTool invoked:", agent_outcome.tool)  
          #print("Tool Input:", agent_outcome.tool_input)
    
          # Print intermediate steps  
          for step in value["intermediate_steps"]:
            # print("Intermediate Step:")
            # print(step.tool, step.tool_input)
            pass
            
        print()

    self.destroy_langraph()
    
    return final_output

  def __init__(self, lab, name):
      
    # Explicitly call parent init
    super().__init__(lab, name)  
    
    # No params required, handled by metaclass
    log_and_print(f"{self.get_name()} in {self.lab.get_name()} initialized. ",
                  LOG_LEVELS.INFO, VERBOSITY_LEVELS.INFO)

    print("\tDefining LAB > ocr_cleanup_tool")
    self.tool = StructuredTool.from_function(
        name="ocr_cleanup_tool",
        func=self.ocr_cleanup,
        description="Cleans up a source text by fixing OCR minor errors and improving readability.",
        handle_tool_error=True,
        verbose=True
    )