#src/sb_slipbox_manager/sb_labs_manager/sb_lab/sb_lab.py

"""
This module defines the sb_lab class which encapsulates a workflow environment powered by specialized LLM agents. The sb_lab is initialized by the sb_labs_manager with a unique name. Within the lab, AI agents can be assigned tasks related to constructing and enhancing the slipbox knowledge base. 

Key agents in the lab include:

_ sb_agent a class that defines how to talk to the LLM, save conversations as a log, etc.

- sb_agent: A subclass of sb_agent, which oversees workflow and assigns tasks to assistant agents.

- sb_validator_assistant: Is tasked with reviewing the work of each assistant, and asking another assistant to implement the changes.

_ a sb_lab_assistant is a sub-class of the sb_agent, which assigns to the assistant their role in the lab.

- sb_extracts_assistant: Is tasked with reviewing an extract generated by Google Vision, and improving it without making changes.

- sb_extracts_agent: Identifies key quotes and ideas

Additonal â€¦ 

- sb_summarizer_agent: Generates slip notes 
- sb_linker_agent: Connects related slips
- sb_tagger_agent: Assigns topics and tags to slips

The lab will give the agent a task, to take source documents as inputs and routes them to agents for processing. The agents leverage LLMs to analyze sources and produce slips, links, tags, and citations.

This modular architecture allows the sb_lab to incrementally build up interconnected slips from sources. New agents can be added to incorporate additional capabilities.

The sb_lab provides a structured environment for AI assistance to augment and automate parts of the slipbox workflow. This allows users to focus on content while the agents handle organization.


The sb_lab class handles:
- Saving the conversation history
- Providing access to prior context
- Passing inputs/outputs between agents
- Conversations stored centralized in the sb_lab rather than dispersed

Additonally it imports GPT options from the constants folder. e.g. GPT Turbo, GPT 4, GPT Local, etc. It also relies on a default.

"""

from src.common import *

from .sb_lab_assistant import sb_lab_assistant

from .sb_lab_validator_assistant import sb_lab_validator_assistant

# >>> IMPORT ASSISTANTS AGENTS HERE <<<<
from .sb_lab_ocr_cleanup_tool import sb_lab_ocr_cleanup_tool
from .sb_lab_metadata_tool import sb_lab_metadata_tool

import json
import datetime
import os

from langchain.schema import AgentAction, AgentFinish

from langchain import hub
from langchain.agents import create_openai_functions_agent
from langchain_openai.chat_models import ChatOpenAI

from langchain_core.runnables import RunnablePassthrough
from langchain_core.agents import AgentFinish

from langgraph.graph import END, Graph

from langchain.globals import set_debug
from langchain.globals import set_verbose

from langchain.agents import Tool
from langchain.agents import AgentType, initialize_agent, load_tools

LAB_TEMPLATE = """Choose from available tools to complete your task. When invoking tools, provide the context of the previous steps (if required). Please return a JSON result. Use Spanish, the language of the source."""

class sb_lab():

  def destroy_langraph(self):

    self.graph = None
    self.chain = None
    self.current_node = None
    self.tools = None
    self.workflow = None
    self.prompt = None
    self.llm = None
    self.agent_runnable = None
    self.agent = None
    self.agent_outcome = None
    self.workflow = None

  # Define the function to execute tools
  def execute_tools(self, data):
      # Get the most recent agent_outcome - this is the key added in the `agent` above
      agent_action = data.pop('agent_outcome')
      # Get the tool to use
      tool_to_use = {t.name: t for t in self.tools}[agent_action.tool]
      

      # Call that tool on the input
      observation = tool_to_use.invoke(agent_action.tool_input)
      # We now add in the action and the observation to the `intermediate_steps` list
      # This is the list of all previous actions taken and their output
      data['intermediate_steps'].append((agent_action, observation))
      return data

  # Define logic that will be used to determine which conditional edge to go down
  def should_continue(self, data):
      # If the agent outcome is an AgentFinish, then we return `exit` string
      # This will be used when setting up the graph to define the flow
      if isinstance(data['agent_outcome'], AgentFinish):
          return "exit"
      # Otherwise, an AgentAction is returned
      # Here we return `continue` string
      # This will be used when setting up the graph to define the flow
      else:
          return "continue"
        
  # Define logic that will be used to determine which conditional edge to go down
  def should_continue_tools(self, data):

      # If the agent outcome is an AgentFinish, then we return `exit` string
      # This will be used when setting up the graph to define the flow
                 
      if isinstance(data['agent_outcome'], AgentFinish):
          return "exit"
      
      # Otherwise, an AgentAction is returned
      # Here we return `continue` string
      # This will be used when setting up the graph to define the flow
      else:
          return "continue"
    
  def get_name(self):
  
    return self.name
      
  def save_conversation(self, agent, agent_prompt, response):
    # Create ID with timestamp
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S%f")[:23]
    convo_id = f"convo-{timestamp}"
    
    # Create conversation dict
    convo = {
      "id": convo_id,
      "agent": agent,
      "prompt": agent_prompt,
      "response": response
     }

    json_convo = json.dumps(convo)

    # Open in append mode (the A)
    with open(self.log_path, "a") as f:
      # Log conversation (will append)
      f.write(f"{json_convo}\n") 

  def __del__(self):
    pass
    
  def do_task(self, task, source):

    task = task
    source = source
    
    log_and_print(f"Initializing the {self.get_name()} lab with the task: {task}",
                  LOG_LEVELS.INFO, VERBOSITY_LEVELS.INFO)

    
    # Turns on LangCahin and Verbosiy for the LangChain    
    set_debug(True)
    set_verbose(True)
 
    from langchain_core.tracers.context import tracing_v2_enabled
    
    # >>> INITIALIZE ASSISTANTS AGENTS HERE <<<<
    # Construct the Lab Agents, that will become nodes for the langraph below.
    # Assistants can be added here. Don't forget to add them to the lang graph below, by adding the node, and then adding edges. Also, add the tool to the tools array,.
    self.ocr_cleanup_tool = sb_lab_ocr_cleanup_tool(self, "ocr_cleanup_tool")
    self.metadata_tool = sb_lab_metadata_tool(self, "metadata_tool")

#    self.assistant_cataloger = sb_lab_assistant_cataloger(self, "catalogue_tool")
#    self.assistant_summarizer = sb_lab_assistant_summarizer(self, "summarize_tool")
#   self.assistant_date_extractor = sb_lab_assistant_date_extractor(self, "date_tool")
#    self.assistant_name_extractor = sb_lab_assistant_name_extractor(self, "name_tool") 
#    self.assistant_place_extractor = sb_lab_assistant_place_extractor(self, "place_tool")

    # >>> ADD THE ASSISTANT AGENTS TOOLS TO THE TOOLS HERE <<<<
    self.tools = [
                  self.ocr_cleanup_tool.get_tool(),
                  self.metadata_tool.get_tool()
    ]

    self.prompt = hub.pull("hwchase17/openai-functions-agent")
  
    """
    # Check the DEFAULT_LLM constant, and use it, and the appropriate model.
    if DEFAULT_LLM == "openai":
        self.llm = ChatOpenAI(model=f"{DEFAULT_LLM_MODEL}")
        
    elif DEFAULT_LLM == "ollama":
        # Replace 'ollama_engine' with the appropriate call to create an ollama engine instance
        self.llm = ollama_engine(model=f"{DEFAULT_LLM_MODEL}")
    else:
        raise ValueError("Unsupported LLM engine specified.")    

    log_and_print(f"Initializing {self.get_name()} afent {self.lab.get_name()} lab with LLM {DEFAULT_LLM} and model {DEFAULT_LLM_MODEL}",
                  LOG_LEVELS.INFO, VERBOSITY_LEVELS.INFO)
    """
    
    # Choose the LLM that will drive the agent
    self.llm = ChatOpenAI(model=f"gpt-4")
    
    # Construct the OpenAI Functions agent
    self.agent_runnable = create_openai_functions_agent(self.llm, self.tools, self.prompt)
    
    # Define the agent
    # Note that here, we are using `.assign` to add the output of the agent to the dictionary
    # This dictionary will be returned from the node
    # The reason we don't want to return just the result of `agent_runnable` from this node is
    # that we want to continue passing around all the other inputs
    self.agent = RunnablePassthrough.assign(
        agent_outcome = self.agent_runnable
    )

    # Define the graph
    self.workflow = Graph()

    # >>> ADD THE AGENTS TOOLS TO THE TOOLS HERE <<<<
    # Add the agent node, we give it name `agent` which we will use later
    # Add a node here. Then add an Edge below.
    self.workflow.add_node("agent", self.agent)
    
    # Add the tools node, we give it name `tools` which we will use later
    self.workflow.add_node("tools", self.execute_tools)
    self.workflow.add_node("ocr_cleanup_tool", self.ocr_cleanup_tool.ocr_cleanup)
    self.workflow.add_node("metadata_tool", self.metadata_tool.metadata)

    # Set the entrypoint as `agent`
    # This means that this node is the first one called
    self.workflow.set_entry_point("agent")

    # We now add a conditional edge
    self.workflow.add_conditional_edges(
        # First, we define the start node. We use `agent`.
        # This means these are the edges taken after the `agent` node is called.
        "agent",
        # Next, we pass in the function that will determine which node is called next.
        self.should_continue,
        # Finally we pass in a mapping.
        # The keys are strings, and the values are other nodes.
        # END is a special node marking that the graph should finish.
        # What will happen is we will call `should_continue`, and then the output of that
        # will be matched against the keys in this mapping.
        # Based on which one it matches, that node will then be called.
        {
            # If `tools`, then we call the tool node.
            "continue": "tools",
            "ocr_cleanup_tool": "ocr_cleanup_tool", 
            "metadata_tool": "metadata_tool",

            # Otherwise we finish.
            "exit": END
        }
    )

    # We now add a normal edge from `tools` to `agent`.
    # This means that after `tools` is called, `agent` node is called next.
    self.workflow.add_edge('tools', 'agent')
    self.workflow.add_edge("ocr_cleanup_tool", "agent")
    self.workflow.add_edge('metadata_tool', 'agent')

    
    # Streaming Node Output
    # chain.invoke({"input": f"""Please choose from the available tools what do to? Please decide what tool to use to pull out the  list of people from this text?\n Here is the text to work with.\n\n{ocr}""", "intermediate_steps": []})

    # Finally, we compile it!
    # This compiles it into a LangChain Runnable,
    # meaning you can use it as you would any other runnable
    self.chain = self.workflow.compile()
    
    final_output = ""
    
    lab_prompt = f"""{LAB_TEMPLATE}

TASKS:    
{task}

SOURCE:
{source}"""
    
    for output in self.chain.stream(
           {"input": f"""{lab_prompt}""", 
           "intermediate_steps": []}
       ):

       for key, value in output.items():

        print(f"Output from LAB Node '{key}':")
        
        # Print original input
        if "input" in value:  
          # print("Original Input:", value["input"])
          pass

        agent_outcome = value.get("agent_outcome")

        if agent_outcome is None:
          continue 

        # Print summary for AgentFinish
        if isinstance(agent_outcome, AgentFinish):
          final_output=agent_outcome.return_values["output"]

        # Print details for AgentAction
        elif isinstance(agent_outcome, AgentAction):
          # print("Agent Prompt:", agent_outcome.log)
          print("\tTool invoked:", agent_outcome.tool)  
          #print("Tool Input:", agent_outcome.tool_input)
    
          # Print intermediate steps  
          for step in value["intermediate_steps"]:
            # print("Intermediate Step:")
            # print(step.tool, step.tool_input)
            pass
            
    self.destroy_langraph()
    
    return final_output
    
  def __init__(self, slipbox_manager, name):

    self.slipbox_manager = slipbox_manager
    self.name = name

    log_and_print(f"Creating the {self.get_name()} lab",
                  LOG_LEVELS.INFO, VERBOSITY_LEVELS.INFO)
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")

    self.log_file_name = f"{LAB_LOG_FILE_NAME}.{format(timestamp)}"
    self.timestamp = timestamp
    
    # Full log path
    self.log_path = f"{DEFAULT_SLIPBOX_PATH}/{DEFAULT_LAB_FOLDER_NAME}/{self.name}/{self.log_file_name}{LAB_LOG_FILE_EXTENSION}"

    # Make folder 
    self.folder_path = os.path.dirname(self.log_path)
    os.makedirs(self.folder_path, exist_ok=True)
