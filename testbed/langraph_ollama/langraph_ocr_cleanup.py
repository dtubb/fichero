# Andy Imports
import asyncio
import typer 
import requests
import srsly
from pathlib import Path
from typing_extensions import Annotated
from rich.progress import track

# Old LangGraph imports
from langchain.schema import AgentAction, AgentFinish
from langchain import hub
from langchain.agents import create_openai_functions_agent

from langchain_openai.chat_models import ChatOpenAI
from langchain_core.agents import AgentFinish
from langgraph.graph import END, Graph
from langchain.globals import set_debug
from langchain.globals import set_verbose
from langchain.agents import Tool
from langchain.agents import AgentType, initialize_agent, load_tools
from langchain.tools import StructuredTool  
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema import AgentAction, AgentFinish

from langchain import hub
from langchain.agents import create_openai_functions_agent
from langchain_openai.chat_models import ChatOpenAI

from langchain_core.runnables import RunnablePassthrough
from langchain_core.agents import AgentFinish

from langgraph.graph import END, Graph

from langchain.globals import set_debug
from langchain.globals import set_verbose

from langchain.agents import AgentType, initialize_agent, load_tools

import inspect
import pprint

# New LangGraph imports
import json
import operator
from typing import Annotated, Sequence, TypedDict
from langchain import hub
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain_community.chat_models import ChatOllama
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.runnables import RunnablePassthrough
from langchain_mistralai.chat_models import ChatMistralAI

local = True
graph = None
chain = None
current_node = None
tools = []
workflow = None
prompt = None
llm = None
agent_runnable = None
agent = None
agent_outcome = None
workflow = None

def ocr_cleanup_tool_function(source):
    
  _TOOL_PROMPT = """Improve the readability of the text extracted by OCR and HDR from an archival document which may have been damaged, have pages torn, have OCR errors, or have sentences out of order. Produce a text faithful to the original OCR, with supported clarifications and no invented data. 
  
- Correct OCR errors, based on surrounding words and language patterns.
- Replace text that is clearly misrecognized, indecipherable, or ambiguous with **UNCLEAR**.
- Only make corrections supported by context, otherwise flag or leave as is.
- Remove irrelevant numbers that appear at the bottom or side, which are the artifact of a ruler.
- SOURCE AND WORKING LANGUAGE IS SPANISH

Format your result as:

REVISED TEXT:

ORIGINAL TEXT: 

The source to clean up, is below:

SOURCE:
{source}
""" 

  prompt = ChatPromptTemplate.from_template(f"{_TOOL_PROMPT}")

  print(local)
  
#  if not local:
#    llm = ChatOllama(model="mistral:7b-instruct", temperature=0)
#  else:
  #llm = ChatOpenAI(model=f"gpt-4")
  llm = ChatOllama(model="mistral:7b-instruct", format="json", temperature=0)
 
  output_parser = StrOutputParser()   

  chain = prompt | llm | output_parser
  
  text = chain.invoke({"source": f"{source}"})

  return text

def ocr_qa_tool_function(source):

  _TOOL_PROMPT = """Validate the corrections made to clarify an text generated by OCR from a document to ensure that changes changes suggested are supported in the context of the original document. Compare the corrected text to the original OCR and note any unsupported changes using these guidelines:
  
- Compare the two texts, and flag any unsupported changes.
- Make sure the language has not been changed. If it has, make sure to flag it.
- Ensure no new content has been added without support from the context of the original text. Flag invented information, supported by the context
- Verify corrections of common OCR errors like "rn" -> "m" are substantiated.
- For issues, describe the issue and recommend actions to resolve issue. 
- BE HARSH. Your job is to be an undertake a quality assurance and validation step on the work of another assistant, which was tasked with clarifying the OCR. You task is to make sure the other AI did a good job.

QUALITY ASSURANCE EVALUATION:

REVISED TEXT:
   
ORIGINAL TEXT:

The previous inputs were as follows:

{source}  
""" 

  prompt = ChatPromptTemplate.from_template(f"{_TOOL_PROMPT}")

  print(local)
  
  #  if not local:
  #    llm = ChatOllama(model="mistral:7b-instruct", temperature=0)
  #  else:
  #llm = ChatOpenAI(model=f"gpt-4")
  llm = ChatOllama(model="mistral:7b-instruct", format="json", temperature=0)

  output_parser = StrOutputParser()   

  chain = prompt | llm | output_parser
  
  text = chain.invoke({"source": f"{source}"})

  return text

def ocr_implement_qa_suggestions_tool_function(source):
    
  _TOOL_PROMPT = """Implement suggestions from a Quality Assurance assistant, which reviewed an earlier version of a cleaned up OCR text. Take the original OCR text, the earlier revisions of the OCR text, the QA suggestions as input, and implement the feedback to improve quality. Please follow these guidelines:

- Make sure the language has not been changed. If it has, make sure to flag to change it back.
- For any flagged issues, make the corrections needed to resolve them, without overcorrecting.
- Only implement changes that are fully substantiated by the context and original text. Avoid guessing if not enough context is provided.
- Use placeholders like **UNCLEAR** when unable to resolve ambiguous text
- Focus narrowly on addressing QA feedback; do not attempt further improvements unprompted.
- If a suggestion seems inappropriate or unjustified, ignore rather than implementing.
- Integrate high-confidence corrections needed to resolve QA issues.
- DO NOT CHANGE THE LANGUAGE OF THE TEXT.
- YOUR WORKING LANGUAGE IS SPANISH

REVISED TEXT BASED ON QUALITY ASSURANCE EVALUATION:

REVISED TEXT:

ORIGINAL TEXT:

The previous input was as follows:

{source}
""" 

  prompt = ChatPromptTemplate.from_template(f"{_TOOL_PROMPT}")

  print(local)
  #  if not local:
  #    llm = ChatOllama(model="mistral:7b-instruct", temperature=0)
  #  else:
  #llm = ChatOpenAI(model=f"gpt-4")
  llm = ChatOllama(model="mistral:7b-instruct", format="json", temperature=0)
  
  output_parser = StrOutputParser()   

  chain = prompt | llm | output_parser
  
  text = chain.invoke({"source": f"{source}"})

  return text

def execute_tools(data):
  # Get the most recent agent_outcome - this is the key added in the `agent` above
  agent_action = data.pop('agent_outcome')
  # Get the tool to use
  tool_to_use = {t.name: t for t in tools}[agent_action.tool]

  # Call that tool on the input
  observation = tool_to_use.invoke(agent_action.tool_input)
  
  # We now add in the action and the observation to the `intermediate_steps` list
  # This is the list of all previous actions taken and their output
  data['intermediate_steps'].append((agent_action, observation))
  
  return data

  # Define logic that will be used to determine which conditional edge to go down

def should_continue(data):
  # If the agent outcome is an AgentFinish, then we return `exit` string
  # This will be used when setting up the graph to define the flow
  if isinstance(data['agent_outcome'], AgentFinish):
      return "exit"
      
  # Otherwise, an AgentAction is returned
  # Here we return `continue` string
  # This will be used when setting up the graph to define the flow

  else:
      return "continue"
      
# Define logic that will be used to determine which conditional edge to go down
def should_continue_tools(data):

  # If the agent outcome is an AgentFinish, then we return `exit` string
  # This will be used when setting up the graph to define the flow
             
  if isinstance(data['agent_outcome'], AgentFinish):
      return "exit"
  
  # Otherwise, an AgentAction is returned
  # Here we return `continue` string
  # This will be used when setting up the graph to define the flow
  else:
      return "continue"

def main(source_file: str,
        run_local: bool):
  
  local = run_local
  source = ""
  
  try:
      with open(source_file, 'r', encoding='utf-8') as file:
          source = file.read()
  except FileNotFoundError:
      print("The file was not found.")
  except Exception as e:
      print(f"An error occurred: {e}")

  # The variable 'file_content' contains the file content as a string
  
  # Gotta Define the tools for the LanGgraph for this OCR Cleanup Tookl
  ocr_cleanup_tool = StructuredTool.from_function(
    name="ocr_cleanup_tool",
    func=ocr_cleanup_tool_function,
    description="Cleans up OCR extracted text by correcting minor errors and improving readability while preserving original meaning and context. Requires TEXT TO CLEANUP.",
    handle_tool_error=True,
    verbose=True
  )

  ocr_qa_tool = StructuredTool.from_function(
    name="ocr_qa_tool",
    func=ocr_qa_tool_function,
    description="Performs a quality assurance check on OCR text revisions, offering suggestions for enhancement. Input needed: REVISED TEXT and ORIGINAL TEXT.",
    handle_tool_error=True,
    verbose=True  
  )

  ocr_implement_qa_suggestions_tool = StructuredTool.from_function(
    name="ocr_implement_qa_suggestions_tool",
    func=ocr_implement_qa_suggestions_tool_function,
    description="Implements QA suggestions to further improve OCR text revisions. Input needed QA EVALUATION, REVISED TEXT, and ORIGINAL TEXT.",
    handle_tool_error=True,
    verbose=True
  )

  # Gotta create a Tools array
  tools = [
       ocr_cleanup_tool,
       ocr_qa_tool,
       ocr_implement_qa_suggestions_tool
  ]
  
  prompt = hub.pull("hwchase17/openai-functions-agent")

  #  if not local:
  #    llm = ChatOllama(model="mistral:7b-instruct", temperature=0)
  #  else:
  #  llm = ChatOpenAI(model=f"gpt-4")
  llm = ChatOllama(model="mistral:7b-instruct", format="json", temperature=0)

  pprint.pprint(f"LLM: '{llm}':")
  
  # Construct the OpenAI Functions agent
  agent_runnable = create_openai_functions_agent(llm, tools, prompt)
  # pprint.pprint(f"agent_runnable: '{agent_runnable}':")  
  
  # Define the agent
  # Note that here, we are using `.assign` to add the output of the agent to the dictionary
  # This dictionary will be returned from the node
  # The reason we don't want to return just the result of `agent_runnable` from this node is
  # that we want to continue passing around all the other inputs
  agent = RunnablePassthrough.assign(
      agent_outcome = agent_runnable
  )
  


  # Define the graph
  workflow = Graph()

  # >>> ADD THE AGENTS TOOLS TO THE TOOLS HERE <<<<
  # Add the agent node, we give it name `agent` which we will use later
  # Add a node here. Then add an Edge below.
  workflow.add_node("agent", agent)
  
  # Add the tools node, we give it name `tools` which we will use later
  workflow.add_node("tools", execute_tools)

  workflow.add_node("OCR_Cleanup_Tool", ocr_cleanup_tool_function)
  workflow.add_node("OCR_QA_Tool", ocr_qa_tool_function)
  workflow.add_node("OCR_Implement_QA_Suggestions_Tool", ocr_implement_qa_suggestions_tool_function)

  # Set the entrypoint as `agent`
  # This means that this node is the first one called
  workflow.set_entry_point("agent")

  # We now add a conditional edge
  workflow.add_conditional_edges(
  "agent",
  should_continue,
  {
      "continue": "tools",
      "OCR_Cleanup_Tool": "OCR_Cleanup_Tool",
      "OCR_QA_Tool": "OCR_QA_Tool",
      "OCR_Implement_QA_Suggestions_Tool": "OCR_Implement_QA_Suggestions_Tool",
      "exit": END
    }
  )

  # We now add a normal edge from `tools` to `agent`.
  # This means that after `tools` is called, `agent` node is called next.
  workflow.add_edge('tools', 'agent')
  workflow.add_edge('OCR_Cleanup_Tool', 'OCR_QA_Tool')
  workflow.add_edge('OCR_QA_Tool', 'OCR_Implement_QA_Suggestions_Tool')
  workflow.add_edge('OCR_Implement_QA_Suggestions_Tool', 'agent')        
  
  chain = workflow.compile()
  
  _TOOL_TEMPLATE = f"""
Improve the OCR of the text, undertake quality assurance on the REVISED TEXT, and implement recommended changes, while  maintaining fidelity to the original. When invoking a tool provide context of previous tools, for reference.

After you are done with the tools, please return only the FINAL REVISED TEXT BASED ON QUALITY ASSURANCE EVALUATION cleaned up OCR text, without the back and forth. 

- REMEMBER DO NOT CHANGE THE LANGUAGE OF THE SOURCE. IT IS IN SPANISH. 
- YOUR WORKING LANGUAGE IS SPANISH

SOURCE:
{source}
"""

  for output in chain.stream(
         {"input": f"""{_TOOL_TEMPLATE}""", 
         "intermediate_steps": []}
     ):

     for key, value in output.items():

      print(f"\n\tOutput from OCR_CLEANUP_TOOL Node '{key}':")
      
      # Print original input
      if "input" in value:  
        # print("Original Input:", value["input"])
        pass

      agent_outcome = value.get("agent_outcome")

      if agent_outcome is None:
        continue 

      # Print summary for AgentFinish
      if isinstance(agent_outcome, AgentFinish):
        output=agent_outcome.return_values["output"]

      # Print details for AgentAction
      elif isinstance(agent_outcome, AgentAction):
        # print("Agent Prompt:", agent_outcome.log)
        print("\tTool invoked:", agent_outcome.tool)  
        #print("Tool Input:", agent_outcome.tool_input)
  
        # Print intermediate steps  
        for step in value["intermediate_steps"]:
          # print("Intermediate Step:")
          # print(step.tool, step.tool_input)
          pass
          
  pprint.pprint(f"Cleanued Source: {output}")

if __name__ == "__main__":
    typer.run(main)